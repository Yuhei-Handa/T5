{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[13959,  1566,    12,  2379,    10,     1,  8947,    19,    78,   207,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "sentence1 = \"translate English to French:\"\n",
    "sentence2 = \"apple is so good\"\n",
    "\n",
    "print(tokenizer(sentence1, sentence2,\n",
    "    padding=\"longest\",\n",
    "\n",
    "    return_tensors=\"pt\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0261,  0.0434, -0.1289, -0.0158,  0.2814, -0.2877, -0.1675, -0.3108,\n",
      "          0.2470,  0.1983],\n",
      "        [-0.0460,  0.1326,  0.2456, -0.2517, -0.2937,  0.2213,  0.0970, -0.2704,\n",
      "         -0.0220, -0.1705],\n",
      "        [ 0.2078, -0.2895, -0.1389, -0.0434, -0.1690,  0.0375,  0.1244, -0.1545,\n",
      "         -0.0084,  0.1417],\n",
      "        [-0.0710, -0.0569, -0.1636,  0.2336, -0.2478, -0.1569, -0.0549, -0.0583,\n",
      "          0.3083, -0.2320],\n",
      "        [-0.2403, -0.2796, -0.2459, -0.1995,  0.1914, -0.2359, -0.1594,  0.2871,\n",
      "         -0.2707, -0.1412]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2308,  0.2195,  0.1126, -0.0944, -0.0218, -0.2988, -0.2593,  0.1112,\n",
      "          0.1572,  0.2109],\n",
      "        [-0.2542, -0.1612, -0.1532,  0.1923, -0.0717, -0.0114, -0.0017, -0.1877,\n",
      "          0.2724,  0.2089],\n",
      "        [ 0.1491, -0.2754, -0.1470,  0.0663, -0.0885, -0.0690, -0.0230,  0.1277,\n",
      "          0.2878,  0.2793],\n",
      "        [-0.1641,  0.0487,  0.1721, -0.0723, -0.0646, -0.0599, -0.3059,  0.2474,\n",
      "         -0.3094, -0.2320],\n",
      "        [ 0.1817,  0.1339, -0.0893,  0.1776,  0.1816, -0.0092,  0.0439,  0.1511,\n",
      "         -0.1716,  0.2219]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1658, -0.2823,  0.0621, -0.2903,  0.2744, -0.0027, -0.0905,  0.1855,\n",
      "         -0.0195,  0.1115],\n",
      "        [ 0.1241,  0.2302,  0.1051, -0.0362,  0.0253, -0.1045,  0.1682, -0.2575,\n",
      "          0.0821,  0.0808],\n",
      "        [ 0.0977, -0.2059, -0.2866,  0.0728,  0.1140, -0.2856,  0.0602, -0.1013,\n",
      "         -0.0328, -0.0700],\n",
      "        [ 0.1413, -0.0338,  0.2453,  0.1440,  0.0253,  0.2938,  0.0336, -0.2539,\n",
      "         -0.1574, -0.1468],\n",
      "        [-0.0414, -0.0634,  0.0967, -0.2869, -0.2417,  0.2452,  0.1811, -0.2949,\n",
      "         -0.2090,  0.1575]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2685,  0.2927,  0.1121, -0.0629, -0.0155,  0.1477,  0.1413,  0.1338,\n",
      "         -0.2046,  0.0682],\n",
      "        [-0.1462, -0.2329,  0.1588,  0.0719, -0.0126, -0.1669,  0.0055, -0.2415,\n",
      "          0.1638,  0.1874],\n",
      "        [ 0.2364, -0.2417, -0.0131, -0.0040, -0.0538,  0.2206, -0.1006,  0.0198,\n",
      "          0.3057,  0.0394],\n",
      "        [ 0.1569, -0.2282,  0.1617,  0.1740,  0.2084,  0.1218,  0.0121,  0.0946,\n",
      "          0.0905,  0.0987],\n",
      "        [ 0.2917, -0.1628, -0.0460, -0.2711, -0.2132, -0.0371,  0.0648,  0.0591,\n",
      "         -0.1795,  0.0270]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0115,  0.2727,  0.0688, -0.3011, -0.2349,  0.0816,  0.1584, -0.1958,\n",
      "          0.0790, -0.1838],\n",
      "        [-0.3002, -0.1842, -0.0901, -0.0818, -0.1578, -0.0278, -0.2417, -0.1458,\n",
      "          0.0916, -0.2593],\n",
      "        [-0.1869, -0.2244,  0.0255, -0.2831,  0.0485,  0.2208,  0.0224, -0.0400,\n",
      "         -0.1312,  0.1918],\n",
      "        [-0.1609, -0.0194, -0.0551, -0.0904,  0.2449,  0.0677, -0.3064,  0.2829,\n",
      "         -0.2528, -0.3110],\n",
      "        [ 0.0598, -0.2486, -0.3084,  0.0496,  0.2518,  0.2744, -0.1190, -0.0322,\n",
      "         -0.1830,  0.1827]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class LINEAR(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.module = self.setup_linear(num)\n",
    "\n",
    "    def setup_linear(self, num):\n",
    "        _list = []\n",
    "        for _ in range(num):\n",
    "            _linear = nn.Linear(10, 5)\n",
    "            _list.append(_linear)\n",
    "\n",
    "        return nn.ModuleList(_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "    \n",
    "l_list = []\n",
    "num = 5\n",
    "for _ in range(num):\n",
    "    l_list.append(LINEAR(num).setup_linear(num))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.ones((3,5), dtype=torch.long)\n",
    "\n",
    "print(torch.stack([a, a],dim=0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [ModuleList] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yuhei\\VScode\\python\\T5\\T5.ipynb セル 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/T5/T5.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m linear_module_sequence \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(linear_module_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/T5/T5.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/T5/T5.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m linear_module_sequence(tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/T5/T5.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mk\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/T5/T5.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:363\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "_list = []\n",
    "\n",
    "for _ in range(5):\n",
    "    linear_module = nn.Linear(5, 5)\n",
    "    _list.append(linear_module)\n",
    "\n",
    "linear_module_list = nn.ModuleList(_list)\n",
    "linear_module_sequence = nn.Sequential(linear_module_list)\n",
    "\n",
    "tensor = torch.ones((5, 5), dtype=torch.float)\n",
    "output = linear_module_sequence(tensor)\n",
    "\n",
    "class k(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _setup(self):\n",
    "        _list = []\n",
    "        for _ in range(5):\n",
    "            linear_module = nn.Linear(5, 5)\n",
    "            _list.append(linear_module)\n",
    "\n",
    "        linear_module_list = nn.ModuleList(_list)\n",
    "        self.linear_module_sequence = nn.Sequential(linear_module_list)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.linear_module_sequence(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "model = k()\n",
    "\n",
    "print(k(tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "\n",
    "class C(A):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "\n",
    "_list = []\n",
    "for _ in range(3):\n",
    "    b= B(\"name\")\n",
    "    c= C(\"name\")\n",
    "    _list.append(b)\n",
    "    _list.append(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"batch_size\":32,\n",
    "    \"seq_length\":64,\n",
    "    \"hidden_size\":768,\n",
    "    \"num_layer\":12,\n",
    "    \"num_heads\":12,\n",
    "    \"ffn_hidden_size\":3072,\n",
    "    \"num_classes\":100\n",
    "}\n",
    "\n",
    "class T5(nn.Module):\n",
    "    def __init__(self, batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_hidden_size = ffn_hidden_size\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.encoder_embedding = nn.Embedding(seq_length + 1, hidden_size)\n",
    "        self.decoder_embedding = nn.Embedding(seq_length + 1, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask):\n",
    "        encoder_embedding = self.encoder_embedding(encoder_input_ids)\n",
    "        output_encoder = self.encoder(encoder_embedding, encoder_attention_mask)\n",
    "        decoder_embedding = decoder_embedding(decoder_input_ids)\n",
    "        decoder_output = self.decoder(decoder_embedding, output_encoder, encoder_attention_mask, decoder_attention_mask)\n",
    "        logits = self.fc(decoder_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class Encoder(T5):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._setupEncoderLayer(self.num_layer)\n",
    "\n",
    "    def _setupEncoderLayer(self, num_layer):\n",
    "        encoder_layer_list = []\n",
    "        for _ in range(num_layer):\n",
    "            encoder_layer = EncoderLayer()\n",
    "            encoder_layer_list.append(encoder_layer)\n",
    "        self.encoder_module = nn.ModuleList(encoder_layer_list)\n",
    "\n",
    "    def forward(self, encoder_embedding, encoder_attention_mask):\n",
    "        tokens = encoder_embedding\n",
    "        for encoder_layer in self.encoder_module:\n",
    "            tokens = encoder_layer(tokens, encoder_attention_mask)\n",
    "        output_encoder = tokens\n",
    "\n",
    "        return output_encoder\n",
    "    \n",
    "class EncoderLayer(Encoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(self.batch_size, self.num_heads, self.seq_length, self.hidden_size, check_positional_embedding=True, check_mask=False)\n",
    "        self.add_norm1 = AddNorm(self.batch_size, self.seq_length, self.hidden_size)\n",
    "        self.feed_forward = FeedForward(self.hidden_size, self.ffn_hidden_size)\n",
    "        self.add_norm2 = AddNorm(self.batch_size, self.seq_length, self.hidden_size)\n",
    "\n",
    "    def forward(self, tokens, encoder_attention_mask):\n",
    "        skip1 = tokens\n",
    "        multi_head_attention = self.multi_head_attention(tokens, tokens, tokens, encoder_attention_mask, encoder_attention_mask)\n",
    "        add_norm1 = self.add_norm1(multi_head_attention, skip1)\n",
    "        skip2 = add_norm1\n",
    "        feed_forward = self.feed_forward(add_norm1)\n",
    "        add_norm2 = self.add_norm2(feed_forward, skip2)\n",
    "        tokens = add_norm2\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class Decoder(T5):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._setupDecoderLayer(self.num_layer)\n",
    "\n",
    "    def _setupDecoderLayer(self, num_layer):\n",
    "        decoder_list = []\n",
    "        for _ in range(num_layer):\n",
    "            decoder_layer = DecoderLayer()\n",
    "            decoder_list.append(decoder_layer)\n",
    "        self.decoder_module = nn.ModuleList(decoder_list)\n",
    "\n",
    "    def forward(self, decoder_embedding, output_encoder, encoder_attention_mask, decoder_attention_mask):\n",
    "        tokens = decoder_embedding\n",
    "        for decoder_layer in self.decoder_module:\n",
    "            tokens = decoder_layer(tokens, output_encoder, encoder_attention_mask, decoder_attention_mask)\n",
    "        output_decoder = tokens\n",
    "\n",
    "        return output_decoder\n",
    "    \n",
    "class DecoderLayer(Decoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention =  MultiHeadAttention(self.batch_size, self.num_heads, self.seq_length, self.hidden_size, check_positional_embedding=True, check_mask=True)\n",
    "        self.add_norm1 = AddNorm(self.batch_size, self.seq_length, self.hidden_size)\n",
    "        self.cross_multi_head_attention = MultiHeadAttention(self.batch_size, self.num_heads, self.seq_length, self.hidden_size, check_positional_embedding=False, check_mask=False)\n",
    "        self.add_norm2 = AddNorm(self.batch_size, self.seq_length, self.hidden_size)\n",
    "        self.feed_forward = FeedForward(self.hidden_size, self.ffn_hidden_size)\n",
    "        self.add_norm3 = AddNorm(self.batch_size, self.seq_length, self.hidden_size)\n",
    "\n",
    "    def forward(self, tokens, output_encoder, encoder_attention_mask, decoder_attention_mask):\n",
    "        skip1 = tokens\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(tokens, tokens, tokens, decoder_attention_mask, decoder_attention_mask)\n",
    "        add_norm1 = self.add_norm1(masked_multi_head_attention, skip1)\n",
    "        skip2 = add_norm1\n",
    "        cross_multi_head_attention = self.cross_multi_head_attention(tokens, output_encoder, output_encoder, decoder_attention_mask, encoder_attention_mask)\n",
    "        add_norm2 = self.add_norm2(cross_multi_head_attention, skip2)\n",
    "        skip3 = add_norm2\n",
    "        feed_forward = self.feed_forward(tokens)\n",
    "        add_norm3 = self.add_norm3(feed_forward, skip3)\n",
    "        tokens = add_norm3\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class MultiHeadAttention(T5):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _setupHeadQKV(self, num_heads, hidden_size):\n",
    "        query_module = []\n",
    "        key_module = []\n",
    "        value_module = []\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "\n",
    "        for _ in range(num_heads):\n",
    "            query_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            key_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            value_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "\n",
    "        self.query_module = nn.ModuleList(query_module)\n",
    "        self.key_module = nn.ModuleList(key_module)\n",
    "        self.value_module = nn.ModuleList(value_module)\n",
    "\n",
    "    def _outputRelativePositionalEmbeddingScalar(self, query, batch_size, seq_length, hidden_size, num_heads):\n",
    "        embed_Module = []\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "        position_ids = torch.tensor(list(range(seq_length)), dtype=torch.long).reshape(1, seq_length).expand(batch_size, seq_length)\n",
    "        for id in range(num_heads): embed_Module.append(nn.Embedding(seq_length + 1, head_hidden_size))\n",
    "        self.embed_module = nn.ModuleList(embed_Module)\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            tmp_relative_position_embedding_scalar = head_query@(self.embed_module[id](position_ids).transpose(1, 2))\n",
    "            if id == 0: relative_position_embedding_scalar = tmp_relative_position_embedding_scalar\n",
    "            else: trelative_position_embedding_scalar = torch.concat([trelative_position_embedding_scalar, tmp_relative_position_embedding_scalar], dim=0)\n",
    "\n",
    "        return relative_position_embedding_scalar\n",
    "    \n",
    "    def _outputAttention(self, query, key, value, batch_size, seq_length, hidden_size, num_heads, check_positional_embedding, check_mask, encoder_attention_mask, decoder_attention_mask):\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "        encoder_attention_mask = encoder_attention_mask.reshape(batch_size, -1, 1).expand(batch_size, seq_length, seq_length)\n",
    "        decoder_attention_mask = decoder_attention_mask.reshape(batch_size, 1, -1).expand(batch_size, seq_length, seq_length)\n",
    "        padding_map = encoder_attention_mask * decoder_attention_mask\n",
    "        mask_map = torch.tensor(np.tril(np.ones((seq_length, seq_length))), dtype=torch.long)\n",
    "\n",
    "        if check_positional_embedding:relative_position_embedding_scalar = self._outputRelativePositionalEmbeddingScalar(query, batch_size, seq_length, hidden_size, num_heads)\n",
    "        else:relative_position_embedding_scalar = torch.tensor(0, torch.float)\n",
    "\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            head_key = self.key_module[id](key)\n",
    "            head_value = self.value_module[id](value)\n",
    "\n",
    "            if check_mask: tmp_head_attention = nn.Softmax(padding_map * mask_map * (head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id])@head_value\n",
    "            else: tmp_head_attention = nn.Softmax(padding_map * (head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id])@head_value\n",
    "\n",
    "            if id == 0: head_attention = tmp_head_attention\n",
    "            else: head_attention = torch.concat([head_attention, tmp_head_attention], dim=-1)\n",
    "\n",
    "        output_attention = head_attention\n",
    "\n",
    "        return output_attention\n",
    "\n",
    "    def forward(self, query, key, value, encoder_attention_mask, decoder_attention_mask):\n",
    "        output_attention = self._outputAttention(self, query, key, value, self.batch_size, self.seq_length, self.hidden_size, self.num_heads,\n",
    "                                                  self.check_positional_embedding, self.check_mask, encoder_attention_mask, decoder_attention_mask):\n",
    "\n",
    "        return output_attention\n",
    "    \n",
    "class AddNorm(T5):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._setupAddNormModule(self.batch_size, self.seq_length, self.hidden_size)\n",
    "\n",
    "    def _setupAddNormModule(self, batch_size, seq_length, hiddne_size):\n",
    "        self.layer_norm = nn.LayerNorm(batch_size, seq_length, hiddne_size)\n",
    "\n",
    "    def forward(self, tokens, skipped_tokens):\n",
    "        tokens += tokens + skipped_tokens\n",
    "        tokens = self.layer_norm(tokens)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class FeedForward(T5):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._setupFeedForwardModule(self.hidden_size, self.ffn_hidden_size)\n",
    "\n",
    "    def _setupFeedForwardModule(self, hidden_size, ffn_hidden_size):\n",
    "        dense1 = nn.Linear(hidden_size, ffn_hidden_size)\n",
    "        relu1 = nn.ReLU()\n",
    "        dense2 = nn.Linear(ffn_hidden_size, hidden_size)\n",
    "        relu2 = nn.ReLU()\n",
    "        self.feed_foward_module = nn.ModuleList([dense1, relu1, dense2, relu2])\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        for module in self.feed_foward_module:\n",
    "            tokens = module(tokens)\n",
    "        return tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
