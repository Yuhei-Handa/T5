{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\yuhei/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\yuhei/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30355,    15,     8,   826,  1566,  1499,   139,  2379,    10,  8774,\n",
      "             6,   149,    33,    25,    58,  2387,    10,     1],\n",
      "        [30355,    15,     8,   826,  1566,  1499,   139,  2379,    10,  1804,\n",
      "          1379,     6,   921,     5,  2387,     1,     0,     0],\n",
      "        [30355,    15,     8,   826,  1566,  1499,   139,  2379,    10,  1072,\n",
      "            25,   199,   140,    28,    48,    58,  2387,     1]])\n",
      "['Bonjour, comment ça va ?',\n",
      " 'Bonjour à tous.',\n",
      " \"Pouvez-vous m'aider avec ceci ?\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from pprint import pprint\n",
    "\n",
    "input = [\"Translate the following English text into French: Hello, how are you? target:\",\n",
    "         \"Translate the following English text into French: Good morning, everyone. target\",\n",
    "          \"Translate the following English text into French: Can you help me with this? target\" ]\n",
    "\n",
    "target = [\"Bonjour, comment ça va ?\",\n",
    "          \"Bonjour à tous.\",\n",
    "          \"Pouvez-vous m'aider avec ceci ?\"]\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "encoder_tokenize = tokenizer(input, return_tensors=\"pt\", padding=True)\n",
    "decoder_tokenize = tokenizer(target, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "encoder_input_ids = encoder_tokenize.input_ids\n",
    "encoder_attention_mask = encoder_tokenize.attention_mask\n",
    "decoder_input_ids = decoder_tokenize.input_ids\n",
    "decoder_attention_mask = decoder_tokenize.attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5(\n",
      "  (encoder_embedding): Embedding(30356, 768, padding_idx=0)\n",
      "  (decoder_embedding): Embedding(30356, 768, padding_idx=0)\n",
      "  (encoder): Encoder(\n",
      "    (encoder_module): ModuleList(\n",
      "      (0-11): 12 x EncoderLayer(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (query_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (key_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (value_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (embed_module): ModuleList(\n",
      "            (0-11): 12 x Embedding(19, 64)\n",
      "          )\n",
      "        )\n",
      "        (add_norm1): AddNorm(\n",
      "          (layer_norm): LayerNorm((3, 18, 768), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (feed_foward_module): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (add_norm2): AddNorm(\n",
      "          (layer_norm): LayerNorm((3, 18, 768), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_module): ModuleList(\n",
      "      (0-11): 12 x DecoderLayer(\n",
      "        (masked_multi_head_attention): MultiHeadAttention(\n",
      "          (query_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (key_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (value_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (embed_module): ModuleList(\n",
      "            (0-11): 12 x Embedding(14, 64)\n",
      "          )\n",
      "        )\n",
      "        (add_norm1): AddNorm(\n",
      "          (layer_norm): LayerNorm((3, 13, 768), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (cross_multi_head_attention): MultiHeadAttention(\n",
      "          (query_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (key_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (value_module): ModuleList(\n",
      "            (0-11): 12 x Linear(in_features=768, out_features=64, bias=True)\n",
      "          )\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (add_norm2): AddNorm(\n",
      "          (layer_norm): LayerNorm((3, 13, 768), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (feed_foward_module): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (add_norm3): AddNorm(\n",
      "          (layer_norm): LayerNorm((3, 13, 768), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "\n",
    "batch_size = encoder_input_ids.size(0)\n",
    "encoder_seq_length = encoder_input_ids.size(1)\n",
    "decoder_seq_length = decoder_input_ids.size(1)\n",
    "\n",
    "kwargs = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_embedding\": max_ids + 1,\n",
    "    \"seq_length\": (encoder_seq_length, decoder_seq_length),\n",
    "    \"hidden_size\": 768,\n",
    "    \"num_layer\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"ffn_hidden_size\": 3072,\n",
    "\n",
    "}\n",
    "\n",
    "class T5(nn.Module):\n",
    "    def __init__(self, batch_size, num_embedding, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_hidden_size = ffn_hidden_size\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n",
    "        self.encoder = Encoder(batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "        self.decoder = Decoder(batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask):\n",
    "        encoder_embedding = self.encoder_embedding(encoder_input_ids)\n",
    "        output_encoder = self.encoder(encoder_embedding, encoder_attention_mask)\n",
    "        decoder_embedding = self.decoder_embedding(decoder_input_ids)\n",
    "        decoder_output = self.decoder(decoder_embedding, output_encoder, encoder_attention_mask, decoder_attention_mask)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self._setupEncoderLayer(batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "    def _setupEncoderLayer(self, batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        encoder_layer_list = []\n",
    "        for _ in range(num_layer):\n",
    "            encoder_layer = EncoderLayer(batch_size, num_heads, seq_length, hidden_size, ffn_hidden_size)\n",
    "            encoder_layer_list.append(encoder_layer)\n",
    "        self.encoder_module = nn.ModuleList(encoder_layer_list)\n",
    "\n",
    "    def forward(self, encoder_embedding, encoder_attention_mask):\n",
    "        tokens = encoder_embedding\n",
    "        for encoder_layer in self.encoder_module:\n",
    "            tokens = encoder_layer(tokens, encoder_attention_mask)\n",
    "        output_encoder = tokens\n",
    "\n",
    "        return output_encoder\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, batch_size, num_heads, seq_length, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(batch_size, num_heads, seq_length, hidden_size, check_positional_embedding=True, check_mask=False)\n",
    "        self.add_norm1 = AddNorm(batch_size, seq_length, hidden_size, check_encoder=True)\n",
    "        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n",
    "        self.add_norm2 = AddNorm(batch_size, seq_length, hidden_size, check_encoder=True)\n",
    "\n",
    "    def forward(self, tokens, encoder_attention_mask):\n",
    "        skip1 = tokens\n",
    "        multi_head_attention = self.multi_head_attention(tokens, tokens, tokens, encoder_attention_mask, encoder_attention_mask)\n",
    "        add_norm1 = self.add_norm1(multi_head_attention, skip1)\n",
    "        skip2 = add_norm1\n",
    "        feed_forward = self.feed_forward(add_norm1)\n",
    "        add_norm2 = self.add_norm2(feed_forward, skip2)\n",
    "        tokens = add_norm2\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self._setupDecoderLayer(batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "    def _setupDecoderLayer(self, batch_size, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        decoder_list = []\n",
    "        for _ in range(num_layer):\n",
    "            decoder_layer = DecoderLayer(batch_size, num_heads, seq_length, hidden_size, ffn_hidden_size)\n",
    "            decoder_list.append(decoder_layer)\n",
    "        self.decoder_module = nn.ModuleList(decoder_list)\n",
    "\n",
    "    def forward(self, decoder_embedding, output_encoder, encoder_attention_mask, decoder_attention_mask):\n",
    "        tokens = decoder_embedding\n",
    "        for decoder_layer in self.decoder_module:\n",
    "            tokens = decoder_layer(tokens, output_encoder, encoder_attention_mask, decoder_attention_mask)\n",
    "        output_decoder = tokens\n",
    "\n",
    "        return output_decoder\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, batch_size, num_heads, seq_length, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention =  MultiHeadAttention(batch_size, num_heads, seq_length, hidden_size, check_positional_embedding=True, check_mask=True)\n",
    "        self.add_norm1 = AddNorm(batch_size, seq_length, hidden_size, check_encoder=False)\n",
    "        self.cross_multi_head_attention = MultiHeadAttention(batch_size, num_heads, seq_length, hidden_size, check_positional_embedding=False, check_mask=False)\n",
    "        self.add_norm2 = AddNorm(batch_size, seq_length, hidden_size, check_encoder=False)\n",
    "        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n",
    "        self.add_norm3 = AddNorm(batch_size, seq_length, hidden_size, check_encoder=False)\n",
    "\n",
    "    def forward(self, tokens, output_encoder, encoder_attention_mask, decoder_attention_mask):\n",
    "        skip1 = tokens\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(tokens, tokens, tokens, decoder_attention_mask, decoder_attention_mask)\n",
    "        add_norm1 = self.add_norm1(masked_multi_head_attention, skip1)\n",
    "        skip2 = add_norm1\n",
    "        cross_multi_head_attention = self.cross_multi_head_attention(tokens, output_encoder, output_encoder, decoder_attention_mask, encoder_attention_mask)\n",
    "        add_norm2 = self.add_norm2(cross_multi_head_attention, skip2)\n",
    "        skip3 = add_norm2\n",
    "        feed_forward = self.feed_forward(tokens)\n",
    "        add_norm3 = self.add_norm3(feed_forward, skip3)\n",
    "        tokens = add_norm3\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, batch_size, num_heads, seq_length, hidden_size, check_positional_embedding, check_mask):\n",
    "        super().__init__()\n",
    "        self._setupHeadQKV(num_heads, hidden_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.check_positional_embedding = check_positional_embedding\n",
    "        self.check_mask = check_mask\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def _setupHeadQKV(self, num_heads, hidden_size):\n",
    "        query_module = []\n",
    "        key_module = []\n",
    "        value_module = []\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "\n",
    "        for _ in range(num_heads):\n",
    "            query_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            key_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            value_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "\n",
    "        self.query_module = nn.ModuleList(query_module)\n",
    "        self.key_module = nn.ModuleList(key_module)\n",
    "        self.value_module = nn.ModuleList(value_module)\n",
    "\n",
    "    def _outputRelativePositionalEmbeddingScalar(self, query, batch_size, seq_length, hidden_size, num_heads):\n",
    "        if self.check_mask: seq_length = self.seq_length[1]\n",
    "        else: seq_length = self.seq_length[0]\n",
    "\n",
    "        embed_Module = []\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "        position_ids = torch.tensor(list(range(seq_length)), dtype=torch.long).reshape(1, seq_length).expand(batch_size, seq_length)\n",
    "        for id in range(num_heads): embed_Module.append(nn.Embedding(seq_length + 1, head_hidden_size))\n",
    "        self.embed_module = nn.ModuleList(embed_Module)\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            tmp_relative_position_embedding_scalar = (head_query@(self.embed_module[id](position_ids).transpose(1, 2))).reshape(1, batch_size, seq_length, seq_length)\n",
    "            if id == 0: relative_position_embedding_scalar = tmp_relative_position_embedding_scalar\n",
    "            else: relative_position_embedding_scalar = torch.concat([relative_position_embedding_scalar, tmp_relative_position_embedding_scalar], dim=0)\n",
    "\n",
    "        return relative_position_embedding_scalar\n",
    "    \n",
    "    def _outputAttention(self, query, key, value, batch_size, seq_length, hidden_size, num_heads, check_positional_embedding, check_mask, encoder_attention_mask, decoder_attention_mask):\n",
    "\n",
    "        if check_positional_embedding:\n",
    "            if check_mask:\n",
    "                seq_length1 = seq_length2 = self.seq_length[1]\n",
    "            else:\n",
    "                seq_length1 = seq_length2 = self.seq_length[0]\n",
    "\n",
    "        else:\n",
    "            seq_length1 = self.seq_length[1]\n",
    "            seq_length2 = self.seq_length[0]\n",
    "\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "        encoder_attention_mask = encoder_attention_mask.reshape(batch_size, -1, 1).expand(batch_size, seq_length1, seq_length2)\n",
    "        decoder_attention_mask = decoder_attention_mask.reshape(batch_size, 1, -1).expand(batch_size, seq_length1, seq_length2)\n",
    "\n",
    "        padding_map = encoder_attention_mask * decoder_attention_mask\n",
    "        mask_map = torch.tensor(np.tril(np.ones((seq_length1, seq_length2))), dtype=torch.long)\n",
    "\n",
    "        if check_positional_embedding:relative_position_embedding_scalar = self._outputRelativePositionalEmbeddingScalar(query, batch_size, seq_length, hidden_size, num_heads)\n",
    "        else: relative_position_embedding_scalar = torch.zeros_like(padding_map, dtype=torch.float).expand(num_heads, batch_size, seq_length1, seq_length2)\n",
    "\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            head_key = self.key_module[id](key)\n",
    "            head_value = self.value_module[id](value)\n",
    "\n",
    "            if check_mask: tmp_head_attention = self.softmax(padding_map * (mask_map * (head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id]))@head_value\n",
    "            else:tmp_head_attention = self.softmax(padding_map * ((head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id]))@head_value\n",
    "            if id == 0: head_attention = tmp_head_attention\n",
    "            else: head_attention = torch.concat([head_attention, tmp_head_attention], dim=-1)\n",
    "        output_attention = head_attention\n",
    "\n",
    "        return output_attention\n",
    "\n",
    "    def forward(self, query, key, value, encoder_attention_mask, decoder_attention_mask):\n",
    "        output_attention = self._outputAttention(query, key, value, self.batch_size, self.seq_length, self.hidden_size, self.num_heads,\n",
    "                                                  self.check_positional_embedding, self.check_mask, encoder_attention_mask, decoder_attention_mask)\n",
    "\n",
    "        return output_attention\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, batch_size, seq_length, hidden_size, check_encoder):\n",
    "        super().__init__()\n",
    "        self._setupAddNormModule(batch_size, seq_length, hidden_size, check_encoder)\n",
    "\n",
    "    def _setupAddNormModule(self, batch_size, seq_length, hidden_size, check_encoder):\n",
    "        if check_encoder: seq_length = seq_length[0]\n",
    "        else: seq_length = seq_length[1]\n",
    "        self.layer_norm = nn.LayerNorm((batch_size, seq_length, hidden_size))\n",
    "\n",
    "    def forward(self, tokens, skipped_tokens):\n",
    "        tokens += skipped_tokens\n",
    "        tokens = self.layer_norm(tokens)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "        self._setupFeedForwardModule(hidden_size, ffn_hidden_size)\n",
    "\n",
    "    def _setupFeedForwardModule(self, hidden_size, ffn_hidden_size):\n",
    "        dense1 = nn.Linear(hidden_size, ffn_hidden_size)\n",
    "        relu1 = nn.ReLU()\n",
    "        dense2 = nn.Linear(ffn_hidden_size, hidden_size)\n",
    "        relu2 = nn.ReLU()\n",
    "        self.feed_foward_module = nn.ModuleList([dense1, relu1, dense2, relu2])\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        for module in self.feed_foward_module:\n",
    "            tokens = module(tokens)\n",
    "        return tokens\n",
    "    \n",
    "model = T5(**kwargs)\n",
    "outputs = model(encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask)\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
